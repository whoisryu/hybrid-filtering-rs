{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow library. Used to implement machine learning models\n",
    "import tensorflow.compat.v1 as tf\n",
    "# Disable the default activate eager execution in TF v1.0\n",
    "tf.disable_eager_execution()\n",
    "#Numpy contains helpful functions for efficient mathematical calculations\n",
    "import numpy as np\n",
    "#Dataframe manipulation library\n",
    "import pandas as pd\n",
    "#Graph plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the movies dataset\n",
    "movies_df =  pd.read_csv('./ml-1m/movies.dat', sep='::', header=None, engine='python',encoding='latin-1')\n",
    "movies_df.columns = ['movieId', 'title', 'genres']\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the ratings dataset\n",
    "ratings_df = pd.read_csv('./ml-1m/ratings.dat', sep='::', header=None, engine='python',encoding='latin-1')\n",
    "ratings_df.columns = ['userId','movieId','rating','timestamp']\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_df = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
    "user_rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_df = pd.read_csv('description.csv')\n",
    "meta_df = pd.read_csv('meta-data.csv')\n",
    "description_df = pd.merge(description_df, meta_df, on=\"movieId\", how=\"inner\")\n",
    "description_df = pd.merge(description_df, movies_df, on='movieId', how='inner')\n",
    "description_df['content'] = description_df['title'].str.replace(r'\\(\\d+\\)', '') + ' ' + description_df['description'] + ' ' + description_df['meta-data']\n",
    "description_df = description_df.drop(['description', 'meta-data'], axis=1)\n",
    "description_df = description_df.dropna()\n",
    "description_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i.encode('ascii', 'ignore').decode() for i in s )\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_df['cleaned'] = description_df['content'].apply(_removeNonAscii)\n",
    "\n",
    "description_df['cleaned'] = description_df.cleaned.apply(func = make_lower_case)\n",
    "description_df['cleaned'] = description_df.cleaned.apply(func = remove_stop_words)\n",
    "description_df['cleaned'] = description_df.cleaned.apply(func=remove_punctuation)\n",
    "description_df['cleaned'] = description_df.cleaned.apply(func=remove_html)\n",
    "description_df = description_df.drop(['content'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for words in description_df['cleaned']:\n",
    "    corpus.append(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load binary Word2Vec model\n",
    "# pretrained_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# pretrained_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "# Create a new Word2Vec model\n",
    "model = Word2Vec(min_count=3,vector_size=750, workers=4,sg=1)\n",
    "\n",
    "# Build the vocabulary from your corpus\n",
    "model.build_vocab(corpus)\n",
    "\n",
    "# Initialize word vectors with the pre-trained model's word vectors\n",
    "# model.wv.vectors = pretrained_model.vectors\n",
    "\n",
    "# Train the model with your corpus\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.0\n",
    "    return dcg_at_k(r, k) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(recommended_items, relevant_items, k):\n",
    "\n",
    "    \n",
    "    recommended_items = recommended_items.iloc[:k]\n",
    "    \n",
    "    # Calculate precision@k\n",
    "    precision = len(set(recommended_items).intersection(set(relevant_items))) / float(k)\n",
    "\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings = ratings_df.groupby('movieId')['rating'].mean()\n",
    "avg_ratings = avg_ratings.rename('avg_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.merge(movies_df, avg_ratings, on='movieId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_item = movies_df[movies_df['avg_rating']>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users =  pd.read_csv('./ml-1m/users.dat', sep='::', header=None, engine='python',encoding='latin-1')\n",
    "users.columns = ['userId', 'gender', 'age', 'occupation', 'zipcode']\n",
    "count_by_user = ratings_df.groupby('userId').size().sort_values(ascending=True)\n",
    "count_by_user=count_by_user.head(10)\n",
    "count_by_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = users['userId'].values\n",
    "\n",
    "# user_ids = count_by_user.index.values\n",
    "user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation(mock_user_id,trXTest):\n",
    "    #Selecting the input user\n",
    "    inputUser = trXTest[mock_user_id-1].reshape(1, -1)\n",
    "\n",
    "    #Feeding in the user and reconstructing the input\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    feed = sess.run(hh0, feed_dict={ v0: inputUser, W: prv_w, hb: prv_hb})\n",
    "    rec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})\n",
    "\n",
    "    scored_movies_df_mock = movies_df[movies_df['movieId'].isin(user_rating_df.columns)]\n",
    "    scored_movies_df_mock = scored_movies_df_mock.assign(recommendationScore = rec[0])\n",
    "    movies_df_mock = ratings_df[ratings_df['userId'] == mock_user_id]\n",
    "\n",
    "    #Merging movies_df with ratings_df by movieId\n",
    "    merged_df_mock = scored_movies_df_mock.merge(movies_df_mock, on='movieId', how='outer')\n",
    "\n",
    "    merged_df_mock_has_watched = merged_df_mock.dropna()\n",
    "    merged_df_mock_has_not_watched = merged_df_mock[merged_df_mock.isna().any(axis=1)]\n",
    "\n",
    "    user_profile = merged_df_mock_has_watched[merged_df_mock_has_watched['rating'] >= 3]\n",
    "    user_profile = user_profile.sort_values(by='rating', ascending=False).head(20)\n",
    "    user_profile = user_profile.drop(['title','recommendationScore', 'userId', 'rating','timestamp','avg_rating'],axis=1)\n",
    "    user_profile.columns = ['movieId','genres']\n",
    "    user_profile_desc = pd.merge(user_profile, description_df, on='movieId')\n",
    "\n",
    "    user_profile_genres = user_profile['genres'].unique()\n",
    "\n",
    "    user_pref = \"\"\n",
    "    for i,data in user_profile_desc.iterrows():\n",
    "        user_pref += \" \" + data['cleaned']\n",
    "\n",
    "    corpus_user_pref = []\n",
    "    corpus_user_pref.append(user_pref.split())\n",
    "    precision=0\n",
    "    recall=0\n",
    "    ndcg=0\n",
    "    if len(corpus_user_pref[0]) > 0:\n",
    "        merged_df_mock.columns = ['movieId','title','genres','avg_rating','recommendationScore','userId','rating','timestamp']\n",
    "        merged_df_mock = merged_df_mock.drop(['genres','rating','userId','timestamp'], axis=1)\n",
    "        # print(merged_df_mock)\n",
    "        merged_df_mock_desc = pd.merge(merged_df_mock,description_df,on='movieId',how='inner')\n",
    "      \n",
    "        merged_df_mock_desc['Similarity Score'] = merged_df_mock_desc.cleaned.apply(lambda x: model.wv.n_similarity(corpus_user_pref[0],x.split()))\n",
    "        \n",
    "        relevance = pd.merge(relevant_item, movies_df_mock, on=\"movieId\", how=\"inner\")\n",
    "        relevance = relevance.drop(['title','genres', 'userId','rating','timestamp'],axis=1)\n",
    "\n",
    "        filtered_df = movies_df[movies_df['genres'].isin(user_profile_genres)]\n",
    "        filtered_genres_df = filtered_df.drop(['title', 'genres'],axis=1)\n",
    "\n",
    "        relevance = pd.concat([relevance,filtered_genres_df ])\n",
    "\n",
    "        recommendationRBM = merged_df_mock_desc.sort_values(by='recommendationScore',ascending=False)\n",
    "        recommendationW2V = merged_df_mock_desc.sort_values(by='Similarity Score',ascending=False)\n",
    "\n",
    "        recommendation_finalRBMW2V =  merged_df_mock_desc.sort_values(by='recommendationScore',ascending=False).head(1000)\n",
    "        recommendation_finalRBMW2V = recommendation_finalRBMW2V.sort_values(by='Similarity Score',ascending=False)\n",
    "\n",
    "        recommendation_finalW2VRbm = merged_df_mock_desc.sort_values(by='Similarity Score',ascending=False).head(1000)\n",
    "        recommendation_finalW2VRbm = recommendation_finalW2VRbm.sort_values(by='recommendationScore',ascending=False)\n",
    "        \n",
    "        k = 20\n",
    "\n",
    "        precisionRBMW2V =  precision_recall_at_k(recommendation_finalRBMW2V['movieId'],relevance['movieId'], k)\n",
    "        precisionW2VRBM = precision_recall_at_k(recommendation_finalW2VRbm['movieId'],relevance['movieId'], k)\n",
    "        precisionRBM = precision_recall_at_k(recommendationRBM['movieId'],relevance['movieId'], k)\n",
    "        precisionW2V = precision_recall_at_k(recommendationW2V['movieId'],relevance['movieId'], k)\n",
    "\n",
    "        # ndcg hybrid\n",
    "        item_ratings1 = recommendation_finalRBMW2V['avg_rating'] # item ratings in the ranked order\n",
    "        ndcgRBMW2V = ndcg_at_k(item_ratings1, k) \n",
    "\n",
    "        item_ratings2 = recommendation_finalW2VRbm['avg_rating'] # item ratings in the ranked order\n",
    "        ndcgW2VRBM = ndcg_at_k(item_ratings2, k) \n",
    "\n",
    "        item_rating3 = recommendationRBM['avg_rating']\n",
    "        ndcgRBM =ndcg_at_k(item_rating3, k) \n",
    "\n",
    "        item_rating4 = recommendationW2V['avg_rating']\n",
    "        ndcgW2V =ndcg_at_k(item_rating4, k) \n",
    "    # return  precisionRBMW2V, ndcgRBMW2V,\n",
    "    return precisionRBMW2V,ndcgRBMW2V, precisionW2VRBM, ndcgW2VRBM, precisionRBM,ndcgRBM,precisionW2V,ndcgW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Initialize the KFold object\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(ratings_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "# Replace the placeholders with your MySQL database details\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='my-secret-pw',\n",
    "    database='tsp'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = user_ids[:3200]\n",
    "user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pRW = []\n",
    "nRW = []\n",
    "pWR = []\n",
    "nWR = []\n",
    "pW = []\n",
    "nW = []\n",
    "pR = []\n",
    "nR = []\n",
    "user_rating_df = train_set.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "\n",
    "norm_user_rating_df = user_rating_df.fillna(0) / 5.0\n",
    "trX = norm_user_rating_df.values\n",
    "\n",
    "hiddenUnits = 400\n",
    "visibleUnits =  len(user_rating_df.columns)\n",
    "vb = tf.placeholder(\"float\", [visibleUnits]) #Number of unique movies\n",
    "hb = tf.placeholder(\"float\", [hiddenUnits]) #Number of features we're going to learn\n",
    "W = tf.placeholder(\"float\", [visibleUnits, hiddenUnits])\n",
    "\n",
    "#Phase 1: Input Processing\n",
    "v0 = tf.placeholder(\"float\", [None, visibleUnits])\n",
    "_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n",
    "h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))\n",
    "#Phase 2: Reconstruction\n",
    "_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) \n",
    "v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))\n",
    "h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)   \n",
    "\n",
    "\n",
    "#Learning rate\n",
    "alpha = 1.0\n",
    "#Create the gradients\n",
    "w_pos_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "w_neg_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "#Calculate the Contrastive Divergence to maximize\n",
    "CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "#Create methods to update the weights and biases\n",
    "update_w = W + alpha * CD\n",
    "update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)\n",
    "update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)\n",
    "\n",
    "err = v0 - v1\n",
    "err_sum = tf.reduce_mean(err * err)\n",
    "\n",
    "#Current weight\n",
    "cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n",
    "#Current visible unit biases\n",
    "cur_vb = np.zeros([visibleUnits], np.float32)\n",
    "#Current hidden unit biases\n",
    "cur_hb = np.zeros([hiddenUnits], np.float32)\n",
    "#Previous weight\n",
    "prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n",
    "#Previous visible unit biases\n",
    "prv_vb = np.zeros([visibleUnits], np.float32)\n",
    "#Previous hidden unit biases\n",
    "prv_hb = np.zeros([hiddenUnits], np.float32)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batchsize = 100\n",
    "errors = []\n",
    "for i in range(epochs):\n",
    "    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch = trX[start:end]\n",
    "        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "        prv_w = cur_w\n",
    "        prv_vb = cur_vb\n",
    "        prv_hb = cur_hb\n",
    "    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))\n",
    "    print (errors[-1])\n",
    "\n",
    "\n",
    "    #Loading in the ratings dataset\n",
    "\n",
    "\n",
    "\n",
    "user_rating_test_df = test_set.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "norm_user_rating_test_df = user_rating_df.fillna(0) / 5.0\n",
    "trXTest = norm_user_rating_df.values\n",
    "i = 1\n",
    "for user in user_ids:\n",
    "    precisionRBMW2V,ndcgRBMW2V, precisionW2VRBM, ndcgW2VRBM, precisionRBM,ndcgRBM,precisionW2V,ndcgW2V = recommendation(user,trXTest)\n",
    "    # precisionW2VRBM, ndcgW2VRBM = recommendation(user,trXTest)\n",
    "    pRW.append(precisionRBMW2V)\n",
    "    nRW.append(ndcgRBMW2V)\n",
    "    pWR.append(precisionW2VRBM)\n",
    "    nWR.append(ndcgW2VRBM)\n",
    "    pR.append(precisionRBM)\n",
    "    nR.append(ndcgRBM)\n",
    "    pW.append(precisionW2V)\n",
    "    nW.append(ndcgW2V)\n",
    "    print(user)\n",
    "    sql = \"INSERT INTO metrics_4 (userid, precisionRW,ndcgRW,precisionWR,ndcgWR,precisionR,ndcgR,precisionW,ndcgW) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    values = (int(user), float(precisionRBMW2V), float(ndcgRBMW2V),float(precisionW2VRBM),float(ndcgW2VRBM),float(precisionRBM),float(ndcgRBM),float(precisionW2V), float(ndcgW2V))\n",
    "    cursor.execute(sql, values)\n",
    "    conn.commit()\n",
    "    print(\"RW \", np.mean(pRW), np.mean(nRW))\n",
    "    print(\"WR \", np.mean(pWR), np.mean(nWR))\n",
    "    print(\"W \", np.mean(pW), np.mean(nW))\n",
    "    print(\"R \", np.mean(pR),np.mean(nR))\n",
    "    print(\"----\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
